<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Margi Pandya - Portfolio</title>
  <link rel="stylesheet" href="../styles.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <!-- Top Navigation Bar -->
  <div class="masthead">
    <div class="masthead__inner-wrap">
      <div class="masthead__menu">
        <nav id="site-nav" class="greedy-nav">
          <button><div class="navicon"></div></button>
          <ul class="visible-links">
            <li class="masthead__menu-item masthead__menu-item--lg">
              <a href="https://margipandya27.github.io/MargiPandya/">Margi Pandya</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/projects.html">Projects</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/notes.html">Blogs</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/publications.html">Publications</a>
            </li>
          </ul>
          <ul class="hidden-links hidden"></ul>
        </nav>
      </div>
    </div>
  </div>
  
  <div class="page-content" style="display: flex; flex-direction: row; max-width: 1300px; margin: 0 auto; padding: 20px;">
  <!-- Sidebar -->
  <div class="sidebar sticky" style="width: 300px; padding: 20px;">
    <div class="mainnotes-sidebar">
      <h3>Content</h3>
      <ul>
          <li><a href="#classical-rag">Classical RAG</a></li>
          <li><a href="#re-ranking-models">With Re-ranking Models</a></li>
          <li><a href="#types-of-re-ranking-models">Types of Re-ranking Models</a></li>
          <li><a href="#colbert">ColBERT</a></li>
          <li><a href="#colbertv2">ColBERTv2</a></li>
          <li><a href="#rankgpt">RankGPT</a></li>
        </ul>
      </div>
    </div>
<!--     </aside> -->
    
    <!-- Main Content -->
    <main style="flex: 1; padding: 20px; max-width: 1000px;">
      <h1>Does Re-ranking Improve the Performance of RAG?</h1>
      <p>
        Yes, re-ranking is a crucial step that can significantly improve the performance of a Retrieval-Augmented Generation (RAG) system. While the initial retrieval stage fetches a broad set of documents, the re-ranking stage refines this set, ensuring that only the most relevant and highest-quality information is passed to the LLM.
      </p>
       <figure class="rag-figure">
        <img src="reranking-scaled.jpg" alt="ColBERT pipeline illustration">
        <figcaption><strong>Figure 1:</strong> End-to-end pipeline of the implemented RAG system.</figcaption>
      </figure>
      <p>Let's look at the process in more detail.</p>

      <hr>

      <h2 id="classical-rag">1. Classical RAG</h2>
      <p>In a traditional RAG pipeline, the process is as follows:</p>
      <ul>
        <li><strong>Retriever:</strong> A dense retriever (e.g., FAISS, DPR) or a sparse retriever (e.g., BM25) fetches the top-k passages.</li>
        <li><strong>Generator:</strong> An LLM (like GPT-4) conditions its response on these k passages to produce an answer.</li>
      </ul>
      <figure class="rag-figure">
        <img src="rag_basic.png" alt="ColBERT pipeline illustration">
        <figcaption><strong>Figure 1:</strong> End-to-end pipeline of the implemented RAG system.</figcaption>
      </figure>
      <p><strong>Problem:</strong></p>
      <ul>
        <li>Increase token input length, leading to higher cost.</li>
        <li>Make the LLMâ€™s reasoning harder, potentially causing hallucinations or irrelevant content.</li>
      </ul>

      <hr>

      <h2 id="re-ranking-models">2. With Re-ranking Models</h2>
      <p>
        Re-ranking is a fine-grained retrieval step that analyzes the relationship between a query and documents more deeply. After the initial retrieval, the top document chunks are passed through a re-ranking model, which re-sorts them in descending order based on their importance to the input query.
      </p>

      <h3 id="types-of-re-ranking-models">Types of Re-ranking Models:</h3>
      <ol>
        <li><strong>Cross-Encoder based models:</strong> ColBERT, ColBERTv2</li>
        <li><strong>LLM-based re-ranking models:</strong> RankGPT</li>
        <li><strong>API-based models:</strong> CohereAPI</li>
      </ol>

      <h3 id="colbert">Cross-Encoder based Models: ColBERT</h3>
  
      <p>ColBERT uses a basic BERT model to encode tokens into embeddings, which is a good choice because:</p>
      <ul>
        <li><strong>Masked Language Modeling (BERT):</strong> ColBERT leverages BERT's Masked Language Modeling, predicting randomly masked tokens with a bidirectional approach (seeing both past and future tokens), resulting in strong contextualized embeddings.</li>
        <li><strong>Causal Modeling (LLM):</strong> In contrast, Causal Modeling predicts the next token in a sequence and is unidirectional. While ideal for generation, the bidirectional embeddings from BERT are more effective for retrieval tasks.</li>
      </ul>
      
      <figure class="rag-figure">
        <img src="Colbert_1.png" alt="ColBERT pipeline illustration">
        <figcaption><strong>Figure 1:</strong> End-to-end pipeline of the implemented RAG system.</figcaption>
      </figure>
      
      <p>The core concept behind <strong>ColBERT</strong> is late interaction. Unlike other models that create a single embedding for a document, ColBERT creates an embedding for every token in the query and every token in the document using a pre-trained BERT-based encoder.</p>
      
      <p>During the re-ranking process, ColBERT calculates a similarity score for every token in the query against all tokens in the documents. It then takes the maximum similarity score for each query token and sums them to determine the document's final relevance score.</p>
      
      <p>The retrieval process in ColBERT is split into two stages:</p>
      
      <p><strong>Approximate Filtering:</strong> For each query embedding, a separate vector similarity search is performed on the FAISS index. The search finds the top-k' most similar document embeddings from the entire collection. The IVFPQ index speeds up this search by searching only a few nearest partitions (p nearest cells). The results of these Nq searches (one for each query embedding) are combined, and the unique document IDs are collected into a set of K candidate documents. These K documents are likely to contain embeddings highly similar to the query embeddings.</p>
      
      <p><strong>Re-ranking and Refinement:</strong> Once the smaller set of K candidate documents is identified, a precise re-ranking finds the final top-k results. The full ColBERT scoring process (MaxSim) is applied to these K documents. The relevance score for each candidate is calculated by summing the maximum similarity between each query embedding and all document embeddings. The documents are then sorted by score, and the top-k are returned as the final result.</p>
      
      <figure class="rag-figure">
        <img src="colbert_result.png" alt="ColBERT re-ranking result">
        <figcaption><strong>Figure 2:</strong> Re-ranking and scoring in the ColBERT system.</figcaption>
      </figure>

    
      <h3 id="colbertv2">ColBERTv2</h3>
      <p>
          <strong>ColBERTv2</strong> is an improvement over ColBERT on two key features: the training strategy and compressed indexing. An initial ColBERT model is used to index the training passages. For each training query, this model retrieves the top-k passages, which often include a mix of relevant documents and challenging, but irrelevant, documents (hard negatives) that are semantically similar to the query.
      </p>
      
      <p>
          <strong>1. Knowledge Distillation</strong><br>
          For training, knowledge distillation of a powerful cross-encoder model like ms-macro-MiniLM-L-6-V2 is used as a teacher model. The scores from the cross-encoder are used to train the ColBERT student model. ColBERTv2 uses a KL-Divergence loss to distill the teacher's scores into its architecture, helping the student model mimic the teacher's ranking distribution. To further improve training, the process also uses in-batch negatives, where a cross-entropy loss is applied to the positive score of each query against all passages corresponding to other queries in the same batch.
      </p>

      <p>
          <strong>2. Indexing</strong><br>
          MaxSim is applied on a reduced inverted list in a two-stage retrieval process to improve efficiency. This two-step process allows for fast candidate generation using approximate scores, followed by a more accurate re-ranking of the top passages.
      </p>
      
      <p>
          <strong>Candidate Generation with the Reduced Inverted List</strong><br>
          This first stage is for speed and efficiency. The goal is to quickly identify a small set of promising documents without decompressing all the vectors.
      </p>
      <ul>
          <li><strong>Query Encoding:</strong> The query is encoded into a set of multi-vector representations, one for each token.</li>
          <li><strong>Centroid Lookup:</strong> For each query token vector, the system finds its nearest nprobe centroids. nprobe is a hyperparameter that controls how many clusters to search.</li>
          <li><strong>Inverted List Traversal:</strong> Using these centroid IDs, the system accesses the inverted list. This list maps each centroid to the compressed passage embeddings that are closest to it.</li>
          <li><strong>Approximate MaxSim:</strong> For each query vector, an approximate "MaxSim" operation is conducted. This involves calculating the similarity between the query vector and the decompressed passage embeddings from the inverted list. This computes a lower-bound on the true MaxSim score, which is a good proxy for relevance.</li>
          <li><strong>Candidate Selection:</strong> The lower-bound scores are summed across the query's tokens to get an overall score for each passage. The top ncandidate passages with the highest approximate scores are selected for the next stage.</li>
      </ul>
      
      <p>
          <strong>Decompression and Full MaxSim Calculation</strong><br>
          For each of the ncandidate passages, the system loads and fully decompresses all of its token embeddings by combining the centroid and the residual vector. The true MaxSim operation is then performed. This involves calculating the cosine similarity between each query token vector and all of the uncompressed passage token vectors, and then summing up the maximum similarities.
      </p>

       <figure class="rag-figure">
        <img src="colbert_v2.png" alt="Alternative text for the image">
        <figcaption><strong>Figure 1:</strong> End-to-end pipeline of the implemented RAG system.</figcaption>
      </figure>

      <h3 id="rankgpt">LLM-based Re-ranking Models: RankGPT</h3>

<p>Instead of the traditional supervised way of fine-tuning a LLM, RankGPT uses a permutation knowledge distillation technique to train the model. This approach involves two main techniques: the sliding window technique for data generation and permutation distillation using RankNet loss.</p>

<p><strong>Sliding Window:</strong> The first step is to generate a large dataset of ranked lists. For a given query and a set of candidate passages, the large LLM (the teacher) uses the instructional permutation and sliding window methods to produce a high-quality, ranked list. This ranked list serves as the ground-truth data for the student model. To overcome the token limits of LLMs, which can't process hundreds of passages at once, the passages are re-ranked in a series of smaller batches using a sliding window. The window moves from the end of the list to the beginning, re-ranking a subset of passages at each step and "locking in" the relative order of the top documents as it proceeds.</p>

<figure class="rag-figure">
  <img src="rankgpt_data_generation.png" alt="RankGPT data generation pipeline">
  <figcaption><strong>Figure 1:</strong> End-to-end pipeline of the implemented RAG system.</figcaption>
</figure>

<p><strong>Permutation Distillation:</strong> The core idea behind RankNet loss is to move beyond simple point-wise relevance scores and instead focus on the relative order of items. It is a type of pairwise loss function used in "learning to rank" algorithms.</p>

<p><strong>Pairwise Comparison:</strong> Instead of treating each passage's relevance score independently, RankNet considers pairs of passages. For a given query, the loss is calculated based on whether the model correctly ranks one passage higher than another, according to the ground-truth ranking provided by the teacher model (e.g., ChatGPT). The objective is to minimize the number of "inversions" in the final ranked list, which occur when a less-relevant passage is ranked above a more-relevant one.</p>

<p><strong>The Loss Function:</strong> The RankNet loss function is:</p>

<blockquote>
  <p>$L_{\text{RankNet}} = \sum_{i=1}^{M} \sum_{j=1}^{M} \mathbb{1}_{r_i < r_j} \log(1+\exp(s_j - s_i))$</p>
</blockquote>

<p>Breaking down the formula:</p>
<ul>
  <li><strong>$M$</strong>: Total number of passages to be ranked for that query.</li>
  <li><strong>$(p_1, ..., p_M)$</strong>: The set of M passages.</li>
  <li><strong>$R=(r_1, ..., r_M)$</strong>: Ground-truth ranking generated by the teacher model. Lower rank numbers indicate higher relevance.</li>
  <li><strong>$s_i = f_{\theta}(q, p_i)$</strong>: Relevance score predicted by the student model for passage $p_i$ given query $q$.</li>
  <li><strong>$\mathbb{1}_{r_i < r_j}$</strong>: Indicator function equal to 1 if $p_i$ is more relevant than $p_j$, 0 otherwise.</li>
  <li><strong>$\log(1+\exp(s_j - s_i))$</strong>: Logistic loss penalizing incorrect ranking; the loss increases when a less-relevant passage is scored higher than a more-relevant one.</li>
</ul>

<p>A smaller, specialized model (the student), such as a BERT-like cross-encoder, is then trained using this generated data. The training objective is not to predict the exact permutation but to produce relevance scores that result in a ranking matching the teacher's. The training uses a pairwise loss function like RankNet loss, penalizing the model if it scores a less-relevant passage higher than a more-relevant one.</p>

      <section class="references">
        <h2>References</h2>
        <ul>
          <li>Khattab, O., & Zaharia, M. (2020). ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. <em>SIGIR</em>.</li>
          <li>Santhanam, K., et al. (2022). ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. <em>arXiv:2112.01488</em>.</li>
          <li>Sun, S., et al. (2023). RankGPT: Instruction Tuning for Generative Ranking. <em>arXiv:2304.09569</em>.</li>
          <li>Cohere AI. (2023). Cohere Rerank API Documentation. Retrieved from <a href="https://docs.cohere.com">https://docs.cohere.com</a>.</li>
        </ul>
      </section>
    </main>
  </div>

  <footer class="page__meta"></footer>
  <div class="page__footer">
    <footer>
      <p>@2025 Margi Pandya</p>
    </footer>
  </div>
</body>
</html>
