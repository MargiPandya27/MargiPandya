<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Margi Pandya - Portfolio</title>
  <link rel="stylesheet" href="../styles.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <!-- Top Navigation Bar -->
  <div class="masthead">
    <div class="masthead__inner-wrap">
      <div class="masthead__menu">
        <nav id="site-nav" class="greedy-nav">
          <button><div class="navicon"></div></button>
          <ul class="visible-links">
            <li class="masthead__menu-item masthead__menu-item--lg">
              <a href="https://margipandya27.github.io/MargiPandya/">Margi Pandya</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/projects.html">Projects</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/notes.html">Blogs</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/publications.html">Publications</a>
            </li>
          </ul>
          <ul class="hidden-links hidden"></ul>
        </nav>
      </div>
    </div>
  </div>
<div class="page-content" style="display: flex; flex-direction: row; max-width: 1300px; margin: 0 auto; padding: 20px;">
  <!-- Sidebar -->
  <div class="sidebar sticky" style="width: 300px; padding: 20px;">
    <div class="mainnotes-sidebar">
      <h3>Main Notes</h3>
      <ul>
          <li><a href="https://margipandya27.github.io/MargiPandya/deep_gen/vae.html">Deep Generative Models</a></li>
          <li><a href="agents.html">Agents</a></li>
          <li><a href="https://margipandya27.github.io/MargiPandya/rag/naive_rag.html">RAGs</a></li>
          <!-- Add more links as needed -->
      </ul>
    </div>
  </div>

<!-- Main Content -->
<main style="flex: 1; padding: 20px; max-width: 1000px;">

  <h1>Does Re-ranking Help Improve the Performance of RAG?</h1>

  <p>
    When evaluating Retrieval-Augmented Generation (RAG), performance should be assessed not only on the quality of generated answers,
    but also on the <strong>quality of retrieved documents</strong>, since retrieved evidence directly shapes the LLM’s outputs.
    In complex applications—especially when the retrieval source is unstructured or noisy (e.g., the open web)—relying only on a
    first-stage retriever is insufficient. This is where <strong>re-ranking</strong> improves performance by refining retrieved passages.
  </p>

  <section>
    <h2>1. Classical RAG (Retriever → Generator)</h2>
    <p>
      In the standard setup:
    </p>
    <ul>
      <li><strong>Retriever:</strong> A dense retriever (e.g., FAISS, DPR, BM25) fetches the top-<em>k</em> passages.</li>
      <li><strong>Generator:</strong> An LLM (such as GPT-4) conditions on these <em>k</em> passages to produce an answer.</li>
    </ul>
    <p>
      <strong>Problem:</strong> The retriever may return many partially relevant or noisy passages. The generator then faces challenges:
    </p>
    <ul>
      <li>Increased input length → higher cost and slower inference.</li>
      <li>Reasoning over noise → higher risk of hallucination or irrelevant mixing.</li>
    </ul>
  </section>

  <section>
    <h2>2. RAG with Re-ranking</h2>
    <p>
      Re-ranking provides a second filtering stage that <em>refines</em> retrieval outputs. After initial retrieval,
      candidate passages are passed through a re-ranking model, which reorders them based on fine-grained similarity to the query.
      This ensures that the generator receives only the <strong>most relevant</strong> evidence.
    </p>
    <p><strong>Types of re-ranking models:</strong></p>
    <ol>
      <li>Cross-encoder based models: <em>ColBERT, ColBERTv2</em></li>
      <li>LLM-based re-ranking: <em>RankGPT</em></li>
      <li>API-based models: <em>Cohere Rerank</em></li>
    </ol>
  </section>

  <section>
    <h2>Cross-Encoder Based Models</h2>
    <h3>ColBERT</h3>
    <p>
      ColBERT is based on BERT encoders. It exploits <strong>masked language modeling</strong> representations,
      which are bidirectional and yield strong embeddings for retrieval and classification.
      Unlike causal models, masked models excel in semantic similarity tasks but are not designed for text generation.
    </p>
    <p>
      ColBERT uses two encoders (for queries and documents). Query tokens are embedded individually,
      and similarity is computed between each query token embedding and document token embeddings.
      For each query token, the maximum similarity across document tokens is taken (<em>MaxSim</em>), and results are summed to form a final score.
      This token-level matching enables fine-grained relevance scoring.
    </p>

    <h4>Approximate Filtering</h4>
    <p>
      To scale, ColBERT uses approximate filtering with FAISS:
    </p>
    <ul>
      <li>Each query embedding runs a similarity search against FAISS indexes (e.g., IVFPQ).</li>
      <li>Nearest partitions are searched, reducing computational cost.</li>
      <li>Results from all query embeddings are merged into <em>K</em> candidate documents.</li>
    </ul>

    <h4>Re-ranking and Refinement</h4>
    <p>
      Once candidates are collected, ColBERT applies the full <em>MaxSim</em> scoring:
    </p>
    <p>
      \[
      \text{Score}(q, d) = \sum_{i} \max_{j} \langle E_{q_i}, E_{d_j} \rangle
      \]
    </p>
    <p>
      Documents are then sorted by these scores, ensuring that top-<em>k</em> passages are the most relevant.
    </p>

    <h3>ColBERTv2</h3>
    <p>
      ColBERTv2 introduces two key improvements:
    </p>
    <ol>
      <li><strong>Training Strategy:</strong> Uses knowledge distillation from a cross-encoder teacher (e.g., ms-macro-MiniLM-L-6-v2).
        KL-divergence loss aligns ColBERT’s scores with teacher distributions, combined with in-batch negatives for robustness.</li>
      <li><strong>Compressed Indexing:</strong> Employs a two-stage retrieval process:
        fast approximate candidate generation with reduced inverted lists, followed by precise <em>MaxSim</em> re-ranking.
      </li>
    </ol>
  </section>

  <section>
    <h2>LLM-Based Re-ranking</h2>
    <h3>RankGPT</h3>
    <p>
      Instead of traditional supervised fine-tuning, RankGPT introduces <strong>permutation knowledge distillation</strong> with two innovations:
    </p>
    <ul>
      <li><strong>Sliding Window:</strong> Handles long candidate lists by re-ranking subsets iteratively, locking in the relative order of top passages.</li>
      <li><strong>Permutation Distillation:</strong> Uses RankNet loss to train a smaller student model (e.g., BERT cross-encoder) from teacher rankings.
    </ul>
    <p>
      The RankNet loss is defined as:
    </p>
    <p>
      \[
      L_{\text{RankNet}} = \sum_{i=1}^M \sum_{j=1}^M \mathbf{1}_{r_i < r_j} \cdot \log(1 + \exp(s_j - s_i))
      \]
    </p>
    <p>
      where \(s_i\) and \(s_j\) are the student model’s scores, and \(r_i, r_j\) are teacher-provided ranks.
      This penalizes inversions (cases where less relevant passages are scored above more relevant ones).
    </p>
  </section>

  <section>
    <h2>API-Based Models</h2>
    <p>
      Commercial APIs like <strong>Cohere Rerank</strong> provide ready-to-use re-ranking functionality optimized for speed and scalability.
      These are useful in production pipelines but introduce dependencies and recurring costs.
    </p>
  </section>

  <section>
    <h2>Discussion</h2>
    <p>
      Re-ranking consistently enhances RAG by reducing noise and ensuring that LLMs receive higher-quality evidence.
      However, the choice of method involves trade-offs:
    </p>
    <ul>
      <li><strong>Cross-encoders:</strong> Accurate, but computationally heavy.</li>
      <li><strong>LLM-based:</strong> Flexible and high-quality, but costly for large-scale use.</li>
      <li><strong>API-based:</strong> Easy to integrate, but dependent on third-party services.</li>
    </ul>
    <p>
      Ultimately, the optimal re-ranking strategy depends on scale, latency requirements, and resource availability.
    </p>
  </section>
</main>


    
<footer class="page__meta">
  <!-- Any additional metadata or footer information goes here -->
</footer>

<div class="page__footer">
  <footer>
    <!-- Your footer content goes here -->
    <p>@2025 Margi Pandya</p>
  </footer>
</div>

</body>
</html>
