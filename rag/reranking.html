<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Margi Pandya - Portfolio</title>
  <link rel="stylesheet" href="../styles.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <!-- Top Navigation Bar -->
  <div class="masthead">
    <div class="masthead__inner-wrap">
      <div class="masthead__menu">
        <nav id="site-nav" class="greedy-nav">
          <button><div class="navicon"></div></button>
          <ul class="visible-links">
            <li class="masthead__menu-item masthead__menu-item--lg">
              <a href="https://margipandya27.github.io/MargiPandya/">Margi Pandya</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/projects.html">Projects</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/notes.html">Blogs</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/publications.html">Publications</a>
            </li>
          </ul>
          <ul class="hidden-links hidden"></ul>
        </nav>
      </div>
    </div>
  </div>
  
  <div class="page-content" style="display: flex; flex-direction: row; max-width: 1300px; margin: 0 auto; padding: 20px;">
  <!-- Sidebar -->
  <div class="sidebar sticky" style="width: 300px; padding: 20px;">
    <div class="mainnotes-sidebar">
      <h3>Content</h3>
      <ul>
          <li><a href="#classical-rag">Classical RAG</a></li>
          <li><a href="#re-ranking-models">With Re-ranking Models</a></li>
          <li><a href="#types-of-re-ranking-models">Types of Re-ranking Models</a></li>
          <li><a href="#colbert">ColBERT</a></li>
          <li><a href="#colbertv2">ColBERTv2</a></li>
          <li><a href="#rankgpt">RankGPT</a></li>
        </ul>
      </div>
    </div>
<!--     </aside> -->
    
    <!-- Main Content -->
    <main style="flex: 1; padding: 20px; max-width: 1000px;">
      <h1>Does Re-ranking Improve the Performance of RAG?</h1>
      <p>
        Yes, re-ranking is a crucial step that can significantly improve the performance of a Retrieval-Augmented Generation (RAG) system. While the initial retrieval stage fetches a broad set of documents, the re-ranking stage refines this set, ensuring that only the most relevant and highest-quality information is passed to the LLM.
      </p>
       <figure class="rag-figure">
        <img src="reranking-scaled.jpg" alt="ColBERT pipeline illustration">
        <figcaption><strong>Figure 1:</strong> End-to-end pipeline of the implemented RAG system.</figcaption>
      </figure>
      <p>Let's look at the process in more detail.</p>

      <hr>

      <h2 id="classical-rag">1. Classical RAG</h2>
      <p>In a traditional RAG pipeline, the process is as follows:</p>
      <ul>
        <li><strong>Retriever:</strong> A dense retriever (e.g., FAISS, DPR) or a sparse retriever (e.g., BM25) fetches the top-k passages.</li>
        <li><strong>Generator:</strong> An LLM (like GPT-4) conditions its response on these k passages to produce an answer.</li>
      </ul>
      <figure class="rag-figure">
        <img src="rag_basic.png" alt="ColBERT pipeline illustration">
        <figcaption><strong>Figure 1:</strong> End-to-end pipeline of the implemented RAG system.</figcaption>
      </figure>
      <p><strong>Problem:</strong></p>
      <ul>
        <li>Increase token input length, leading to higher cost.</li>
        <li>Make the LLMâ€™s reasoning harder, potentially causing hallucinations or irrelevant content.</li>
      </ul>

      <hr>

      <h2 id="re-ranking-models">2. With Re-ranking Models</h2>
      <p>
        Re-ranking is a fine-grained retrieval step that analyzes the relationship between a query and documents more deeply. After the initial retrieval, the top document chunks are passed through a re-ranking model, which re-sorts them in descending order based on their importance to the input query.
      </p>

      <h3 id="types-of-re-ranking-models">Types of Re-ranking Models:</h3>
      <ol>
        <li><strong>Cross-Encoder based models:</strong> ColBERT, ColBERTv2</li>
        <li><strong>LLM-based re-ranking models:</strong> RankGPT</li>
        <li><strong>API-based models:</strong> CohereAPI</li>
      </ol>

      <h3 id="colbert">Cross-Encoder based Models: ColBERT</h3>
  
      <p>ColBERT uses a basic BERT model to encode tokens into embeddings, which is a good choice because:</p>
      <ul>
        <li><strong>Masked Language Modeling (BERT):</strong> ColBERT leverages BERT's Masked Language Modeling, predicting randomly masked tokens with a bidirectional approach (seeing both past and future tokens), resulting in strong contextualized embeddings.</li>
        <li><strong>Causal Modeling (LLM):</strong> In contrast, Causal Modeling predicts the next token in a sequence and is unidirectional. While ideal for generation, the bidirectional embeddings from BERT are more effective for retrieval tasks.</li>
      </ul>
      
      <figure class="rag-figure">
        <img src="Colbert_1.png" alt="ColBERT pipeline illustration">
        <figcaption><strong>Figure 1:</strong> End-to-end pipeline of the implemented RAG system.</figcaption>
      </figure>
      
      <p>The core concept behind <strong>ColBERT</strong> is late interaction. Unlike other models that create a single embedding for a document, ColBERT creates an embedding for every token in the query and every token in the document using a pre-trained BERT-based encoder.</p>
      
      <p>During the re-ranking process, ColBERT calculates a similarity score for every token in the query against all tokens in the documents. It then takes the maximum similarity score for each query token and sums them to determine the document's final relevance score.</p>
      
      <p>The retrieval process in ColBERT is split into two stages:</p>
      
      <p><strong>Approximate Filtering:</strong> For each query embedding, a separate vector similarity search is performed on the FAISS index. The search finds the top-k' most similar document embeddings from the entire collection. The IVFPQ index speeds up this search by searching only a few nearest partitions (p nearest cells). The results of these Nq searches (one for each query embedding) are combined, and the unique document IDs are collected into a set of K candidate documents. These K documents are likely to contain embeddings highly similar to the query embeddings.</p>
      
      <p><strong>Re-ranking and Refinement:</strong> Once the smaller set of K candidate documents is identified, a precise re-ranking finds the final top-k results. The full ColBERT scoring process (MaxSim) is applied to these K documents. The relevance score for each candidate is calculated by summing the maximum similarity between each query embedding and all document embeddings. The documents are then sorted by score, and the top-k are returned as the final result.</p>
      
      <figure class="rag-figure">
        <img src="colbert_result.png" alt="ColBERT re-ranking result">
        <figcaption><strong>Figure 2:</strong> Re-ranking and scoring in the ColBERT system.</figcaption>
      </figure>

    
      <h3 id="colbertv2">ColBERTv2</h3>
      <p>
          <strong>ColBERTv2</strong> is an improvement over ColBERT on two key features: the training strategy and compressed indexing. An initial ColBERT model is used to index the training passages. For each training query, this model retrieves the top-k passages, which often include a mix of relevant documents and challenging, but irrelevant, documents (hard negatives) that are semantically similar to the query.
      </p>
      
      <p>
          <strong>1. Knowledge Distillation</strong><br>
          For training, knowledge distillation of a powerful cross-encoder model like ms-macro-MiniLM-L-6-V2 is used as a teacher model. The scores from the cross-encoder are used to train the ColBERT student model. ColBERTv2 uses a KL-Divergence loss to distill the teacher's scores into its architecture, helping the student model mimic the teacher's ranking distribution. To further improve training, the process also uses in-batch negatives, where a cross-entropy loss is applied to the positive score of each query against all passages corresponding to other queries in the same batch.
      </p>

      <p>
          <strong>2. Indexing</strong><br>
          MaxSim is applied on a reduced inverted list in a two-stage retrieval process to improve efficiency. This two-step process allows for fast candidate generation using approximate scores, followed by a more accurate re-ranking of the top passages.
      </p>
      
      <p>
          <strong>Candidate Generation with the Reduced Inverted List</strong><br>
          This first stage is for speed and efficiency. The goal is to quickly identify a small set of promising documents without decompressing all the vectors.
      </p>
      <ul>
          <li><strong>Query Encoding:</strong> The query is encoded into a set of multi-vector representations, one for each token.</li>
          <li><strong>Centroid Lookup:</strong> For each query token vector, the system finds its nearest nprobe centroids. nprobe is a hyperparameter that controls how many clusters to search.</li>
          <li><strong>Inverted List Traversal:</strong> Using these centroid IDs, the system accesses the inverted list. This list maps each centroid to the compressed passage embeddings that are closest to it.</li>
          <li><strong>Approximate MaxSim:</strong> For each query vector, an approximate "MaxSim" operation is conducted. This involves calculating the similarity between the query vector and the decompressed passage embeddings from the inverted list. This computes a lower-bound on the true MaxSim score, which is a good proxy for relevance.</li>
          <li><strong>Candidate Selection:</strong> The lower-bound scores are summed across the query's tokens to get an overall score for each passage. The top ncandidate passages with the highest approximate scores are selected for the next stage.</li>
      </ul>
      
      <p>
          <strong>Decompression and Full MaxSim Calculation</strong><br>
          For each of the ncandidate passages, the system loads and fully decompresses all of its token embeddings by combining the centroid and the residual vector. The true MaxSim operation is then performed. This involves calculating the cosine similarity between each query token vector and all of the uncompressed passage token vectors, and then summing up the maximum similarities.
      </p>

       <figure class="rag-figure">
        <img src="colbert_v2.png" alt="Alternative text for the image">
        <figcaption><strong>Figure 1:</strong> End-to-end pipeline of the implemented RAG system.</figcaption>
      </figure>

   <h3 id="rankgpt">LLM-based Re-ranking Models: RankGPT</h3>
        
        <p>RankGPT uses <em>permutation knowledge distillation</em> instead of traditional supervised fine-tuning. The approach has two main components: <strong>sliding window data generation</strong> and <strong>permutation distillation</strong> with RankNet loss.</p>
        
        <p><strong>Sliding Window:</strong></p>
        <p>Given a query <em>q</em> and a candidate set of passages 
        $\mathcal{P} = \{p_1, p_2, \dots, p_M\}$, the teacher LLM produces a high-quality ranked list:</p>
        
        <p style="text-align:center">$$R = \{r_1, r_2, \dots, r_M\}, \quad r_i = \text{rank}(p_i)$$</p>
        
        <p>Since LLMs cannot process all $M$ passages at once, passages are split into overlapping windows of size $w$:</p>
        
        <p style="text-align:center">$$\mathcal{W}_k = \{p_k, p_{k+1}, \dots, p_{k+w-1}\}, \quad k=1, \dots, M-w+1$$</p>
        
        <p>The teacher re-ranks passages inside each window. At step $k$, the relative order of the top-$t$ passages is <em>locked in</em> while the window slides backward through the list.</p>
        
        <figure class="rag-figure">
          <img src="rankgpt_data_generation.png" alt="RankGPT data generation pipeline">
          <figcaption><strong>Figure 1:</strong> Data generation using sliding windows.</figcaption>
        </figure>
        
        <p><strong>Permutation Distillation:</strong></p>
        <p>The student model $f_\theta(q, p_i) = s_i$ assigns a score $s_i$ to each passage $p_i$. Instead of absolute scores, RankNet uses <em>pairwise ordering</em>:</p>
        
        <p style="text-align:center">$$L_{\text{RankNet}} = \sum_{i=1}^{M}\sum_{j=1}^{M} \mathbf{1}_{r_i < r_j} \, \log\!\big(1 + \exp(s_j - s_i)\big)$$</p>
        
        <p>Where:</p>
        <ul>
          <li>$M$: number of passages</li>
          <li>$r_i$: rank of passage $p_i$ from teacher</li>
          <li>$s_i$: predicted score from student</li>
          <li>$\mathbf{1}_{r_i < r_j}$: indicator if passage $p_i$ is more relevant than $p_j$</li>
        </ul>
        
        <p>Intuitively:</p>
        <ul>
          <li>If $r_i < r_j$ (i.e., $p_i$ is more relevant), we want $s_i > s_j$.</li>
          <li>If the student gets it wrong ($s_j > s_i$), then $(s_j - s_i) > 0$ and the loss increases.</li>
          <li>If it gets it right ($s_i > s_j$), then $(s_j - s_i) < 0$ and the loss approaches $0$.</li>
        </ul>
        
        <p>Thus, the student (e.g., a BERT cross-encoder) learns to approximate the teacherâ€™s ranking by minimizing pairwise inversions rather than predicting exact permutations.</p>

      <section class="references">
        <h2>References</h2>
        <ul>
          <li>Khattab, O., & Zaharia, M. (2020). ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. <em>SIGIR</em>.</li>
          <li>Santhanam, K., et al. (2022). ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. <em>arXiv:2112.01488</em>.</li>
          <li>Sun, S., et al. (2023). RankGPT: Instruction Tuning for Generative Ranking. <em>arXiv:2304.09569</em>.</li>
          <li>Cohere AI. (2023). Cohere Rerank API Documentation. Retrieved from <a href="https://docs.cohere.com">https://docs.cohere.com</a>.</li>
        </ul>
      </section>
    </main>
  </div>

  <footer class="page__meta"></footer>
  <div class="page__footer">
    <footer>
      <p>@2025 Margi Pandya</p>
    </footer>
  </div>
</body>
</html>
