<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Margi Pandya - Portfolio</title>
  <link rel="stylesheet" href="../styles.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <!-- Top Navigation Bar -->
  <div class="masthead">
    <div class="masthead__inner-wrap">
      <div class="masthead__menu">
        <nav id="site-nav" class="greedy-nav">
          <button><div class="navicon"></div></button>
          <ul class="visible-links">
            <li class="masthead__menu-item masthead__menu-item--lg">
              <a href="https://margipandya27.github.io/MargiPandya/">Margi Pandya</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/projects.html">Projects</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/notes.html">Blogs</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/publications.html">Publications</a>
            </li>
          </ul>
          <ul class="hidden-links hidden"></ul>
        </nav>
      </div>
    </div>
  </div>
<div class="page-content" style="display: flex; flex-direction: row; max-width: 1300px; margin: 0 auto; padding: 20px;">
  <!-- Sidebar -->
  <div class="sidebar sticky" style="width: 300px; padding: 20px;">
    <div class="mainnotes-sidebar">
      <h3>Main Notes</h3>
      <ul>
          <li><a href="https://margipandya27.github.io/MargiPandya/deep_gen/vae.html">Deep Generative Models</a></li>
          <li><a href="agents.html">Agents</a></li>
          <li><a href="https://margipandya27.github.io/MargiPandya/rag/naive_rag.html">RAGs</a></li>
          <!-- Add more links as needed -->
      </ul>
    </div>
  </div>

<!-- Main Content -->
  <main style="flex: 1; padding: 20px; max-width: 1000px;">
     <body>
    <article class="container">
        <h1>Does Re-ranking Improve the Performance of RAG?</h1>
        <p>
            Yes, re-ranking is a crucial step that can significantly improve the performance of a Retrieval-Augmented Generation (RAG) system. While the initial retrieval stage fetches a broad set of documents, the re-ranking stage refines this set, ensuring that only the most relevant and highest-quality information is passed to the LLM.
        </p>
        <p>
            Let's look at the process in more detail.
        </p>

        <hr>

        <h2>1. Classical RAG (Retriever → Generator)</h2>
        <p>
            In a traditional RAG pipeline, the process is as follows:
        </p>
        <ul>
            <li><strong>Retriever:</strong> A dense retriever (e.g., FAISS, DPR) or a sparse retriever (e.g., BM25) fetches the top-k passages.</li>
            <li><strong>Generator:</strong> An LLM (like GPT-4) conditions its response on these k passages to produce an answer.</li>
        </ul>
        <p>
            <strong>Problem:</strong>
        </p>
        <p>
            This approach has limitations. The initial retriever may bring back many partially relevant or noisy passages. The LLM then has to process all of them, which can:
        </p>
        <ul>
            <li>Increase token input length, leading to higher cost.</li>
            <li>Make the LLM’s reasoning harder, potentially causing it to hallucinate or mix in irrelevant content.</li>
        </ul>

        <hr>

        <h2>2. With Re-ranking Models</h2>
        <p>
            Re-ranking is a fine-grained retrieval step that analyzes the relationship between a query and documents more deeply. After the initial retrieval, the top document chunks are passed through a re-ranking model, which re-sorts them in descending order based on their importance to the input query.
        </p>
        
        <h3>Types of Re-ranking Models:</h3>
        <ol>
            <li><strong>Cross-Encoder based models:</strong> ColBERT, ColBERTv2</li>
            <li><strong>LLM-based re-ranking models:</strong> RankGPT</li>
            <li><strong>API-based models:</strong> CohereAPI</li>
        </ol>

        <h3>Cross-Encoder based Models:</h3>
        <h4>ColBERT</h4>
        <p>
            The core concept behind <strong>ColBERT</strong> is that of late interaction. Unlike other models that create a single embedding for a document, ColBERT creates a powerful embedding for every token in the query and every token in the document using a pre-trained BERT-based encoder.
        </p>
        <ul>
            <li><strong>Masked Language Modeling (BERT):</strong> ColBERT leverages the power of BERT's Masked Language Modeling, which predicts randomly masked tokens and uses a bidirectional approach (seeing both past and future tokens). This results in strong contextualized embeddings.</li>
            <li><strong>Causal Modeling (LLM):</strong> In contrast, Causal Modeling predicts the next token in a sequence and is unidirectional. While ideal for generation, the powerful bidirectional embeddings from BERT are more effective for retrieval tasks.</li>
        </ul>
        <p>
            During the re-ranking process, ColBERT efficiently calculates a similarity score for every token in the query against all tokens in the documents. It then takes the maximum similarity score for each query token and sums them up to determine the document's final relevance score.
        </p>
        <p>
            The retrieval process in ColBERT is split into two stages:
        </p>
        <ol>
            <li><strong>Approximate Filtering:</strong> For each of the query's token embeddings, a separate vector similarity search is performed on an index (like FAISS). This quickly finds the top-k' most similar document embeddings from the entire collection for each query embedding. The results are combined into a set of candidate documents.</li>
            <li><strong>Re-ranking and Refinement:</strong> On this smaller set of candidate documents, the full ColBERT scoring process (MaxSim) is performed. The relevance score for each candidate is calculated by taking the sum of the maximum similarity between each query embedding and all of the document's embeddings. The documents are then sorted by their scores, and the final top-k are returned.</li>
        </ol>

        <h4>ColBERTv2</h4>
        <p>
            <strong>ColBERTv2</strong> is an improvement over ColBERT with two new key features: a new training strategy and compressed indexing. An initial ColBERT model is used to index the training passages, and for each query, it retrieves a mix of relevant and challenging irrelevant documents.
        </p>
        <ol>
            <li><strong>Knowledge Distillation:</strong> For training, an already trained ColBERT model acts as a "student," and its scores are compared to a more powerful "teacher" model (a cross-encoder). ColBERTv2 uses a KL-Divergence loss to distill the teacher's ranking distribution into its architecture. This helps the student model mimic the teacher's ranking behavior.</li>
            <li><strong>Compressed Indexing:</strong> ColBERTv2 applies its MaxSim scoring on a reduced inverted list to improve efficiency. This two-step process allows for fast candidate generation using approximate scores, followed by a more accurate re-ranking of the top passages, which are then fully decompressed for a precise MaxSim calculation.</li>
        </ol>

        <h3>LLM-based Re-ranking Models: RankGPT</h3>
        <p>
            Instead of traditional supervised fine-tuning, <strong>RankGPT</strong> uses a permutation knowledge distillation technique to train a model. It uses a large LLM as a "teacher" to generate a dataset of ranked lists, which are then used to train a smaller "student" model.
        </p>
        <p>
            This is done using two interesting techniques:
        </p>
        <ol>
            <li><strong>Sliding Window:</strong> To overcome the token limits of large LLMs, which cannot process hundreds of passages at once, the passages are re-ranked in a series of smaller batches using a sliding window. The window moves from the end of the list to the beginning, re-ranking a subset of passages at each step and "locking in" the relative order of the top documents as it proceeds.</li>
            <li><strong>Permutation Distillation:</strong> The core idea is to train the smaller model to get the relative order of documents correct, rather than their exact relevance score. It uses a pairwise loss function known as <strong>RankNet loss</strong>, which measures the correctness of the relative ordering.</li>
        </ol>

        <p>The RankNet loss function is defined as:</p>
        <blockquote>
            <p>
                $L_{\text{RankNet}} = \sum_{i=1}^{M} \sum_{j=1}^{M} \mathbb{1}_{r_i < r_j} \log(1+\exp(s_j - s_i))$
            </p>
        </blockquote>
        <p>Let's break down this formula:</p>
        <ul>
            <li><strong>$r_i < r_j$</strong>: An indicator function that is 1 if passage $i$ is more relevant than passage $j$ (i.e., its rank $r_i$ is lower than $r_j$), and 0 otherwise.</li>
            <li><strong>$s_i$ and $s_j$</strong>: The relevance scores predicted by the student model for passages $p_i$ and $p_j$.</li>
            <li><strong>$\log(1+\exp(s_j - s_i))$</strong>: The logistic loss that penalizes the model when its predicted scores do not match the correct order. If the student model incorrectly scores a less-relevant passage ($s_j$) higher than a more-relevant one ($s_i$), the term $(s_j - s_i)$ will be a positive value, leading to a high loss. When the model gets the order right ($s_i > s_j$), the term is a large negative number, and the loss approaches zero.</li>
        </ul>
        <p>
            This method allows a smaller, specialized model to be trained to match the ranking quality of a much larger, more expensive LLM.
        </p>
    </article>
</body>
</main>

    
<footer class="page__meta">
  <!-- Any additional metadata or footer information goes here -->
</footer>

<div class="page__footer">
  <footer>
    <!-- Your footer content goes here -->
    <p>@2025 Margi Pandya</p>
  </footer>
</div>

</body>
</html>
