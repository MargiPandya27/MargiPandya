<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Margi Pandya - Portfolio</title>
  <link rel="stylesheet" href="../styles.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <!-- Top Navigation Bar -->
  <div class="masthead">
    <div class="masthead__inner-wrap">
      <div class="masthead__menu">
        <nav id="site-nav" class="greedy-nav">
          <button><div class="navicon"></div></button>
          <ul class="visible-links">
            <li class="masthead__menu-item masthead__menu-item--lg">
              <a href="https://margipandya27.github.io/MargiPandya/">Margi Pandya</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/projects.html">Projects</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/notes.html">Blogs</a>
            </li>
            <li class="masthead__menu-item">
              <a href="https://margipandya27.github.io/MargiPandya/publications.html">Publications</a>
            </li>
          </ul>
          <ul class="hidden-links hidden"></ul>
        </nav>
      </div>
    </div>
  </div>
<div class="page-content" style="display: flex; flex-direction: row; max-width: 1300px; margin: 0 auto; padding: 20px;">
  <!-- Sidebar -->
  <div class="sidebar sticky" style="width: 300px; padding: 20px;">
    <div class="mainnotes-sidebar">
      <h3>Main Notes</h3>
      <ul>
          <li><a href="https://margipandya27.github.io/MargiPandya/deep_gen/vae.html">Deep Generative Models</a></li>
          <li><a href="agents.html">Agents</a></li>
          <li><a href="https://margipandya27.github.io/MargiPandya/rag/naive_rag.html">RAGs</a></li>
          <!-- Add more links as needed -->
      </ul>
    </div>
  </div>

<!-- Main Content -->
  <main style="flex: 1; padding: 20px; max-width: 1000px;">
      <h1>Does Re-ranking Help Improve the Performance of RAG?</h1>

      <section id="overview">
        <h2>Overview</h2>
        <p>
          When it comes to evaluation of RAG, it should be evaluated on the quality of retrieved documents,
          since the quality of the retrieved documents determines the quality of the generated response by the LLM.
          For RAG with complex applications, and also when the source of retrieval is unknown webpages,
          then after performing only the first stage of retrieval would not be helpful.
        </p>
      </section>

      <section id="classical-rag">
        <h2>1. Classical RAG (Retriever → Generator)</h2>
        <p><strong>Retriever:</strong> dense retriever (e.g., FAISS, DPR, BM25) fetches top-k passages.</p>
        <p><strong>Generator:</strong> LLM (like GPT-4) conditions on these k passages and produces an answer.</p>
        <p><strong>Problem:</strong> Retriever may bring back many partially relevant or noisy passages. Generator has to process all of them, which:</p>
        <ul>
          <li>Increases token input length (higher cost).</li>
          <li>Makes the LLM’s reasoning harder → it may hallucinate or mix irrelevant content.</li>
        </ul>
      </section>

      <section id="with-reranking">
        <h2>2. With Re-ranking Models</h2>
        <p>
          Re-ranking is a way of fine-grained retrieval results, that can search for individual words
          in query and documents at the same time and jointly. After rough retrieval,
          the document chunks are passed through the re-ranking model, which are arranged
          descendingly based on the importance with the input query.
        </p>
        <p><strong>Types of Re-ranking Models:</strong></p>
        <ol>
          <li>Cross-Encoder based models: ColBERT, ColBERTv2</li>
          <li>LLM based re-ranking models: RankGPT</li>
          <li>API based models: Cohere API</li>
        </ol>
      </section>

      <section id="cross-encoder">
        <h2>Cross-Encoder Based Models</h2>
        <h3>ColBERT</h3>
        <p>
          ColBERT is based on a BERT encoder. The main concept behind ColBERT is that of masked language modelling in BERT.
        </p>
        <p><strong>Masked Modelling:</strong></p>
        <ul>
          <li>Predict randomly masked tokens</li>
          <li>Bidirectional (sees past and future tokens)</li>
          <li>Strength: strong embeddings</li>
          <li>Useful for retrieval and classification tasks</li>
          <li>Limitation: not useful for generation</li>
        </ul>
        <p><strong>Causal Modelling:</strong></p>
        <ul>
          <li>Predict next token in sequence</li>
          <li>Unidirectional (only sees past tokens)</li>
          <li>Strength: compatible with autoregressive LLMs</li>
          <li>Useful for generation, instruction-following tasks</li>
          <li>Limitation: harder to train (less parallelism)</li>
        </ul>
        <p>
          The ColBERT model has two encoders for queries and documents which are pre-trained.
          Offline, the similarity score is computed for every token in query and all the documents.
          The max similarity across tokens is taken for each document, then arranged in descending order.
        </p>
        <p>
          They make use of the powerful embeddings of the BERT model in masked modelling.
          Query embeddings are tokenized into <code>q1, q2, q3, ...</code> and converted to bag-of-words embeddings.
        </p>

        <h4>Approximate Filtering</h4>
        <p>
          For each of the query’s embeddings (\(E_q\)), a separate vector similarity search is performed on the FAISS index.
          The search finds the top-k′ most similar document embeddings. The IVFPQ index is used to speed this up.
        </p>

        <h4>Re-ranking and Refinement</h4>
        <p>
          Once the smaller set of K candidate documents is identified, ColBERT performs precise re-ranking.
          Using late interaction, the relevance score is calculated as:
        </p>
        <p>
          \[
          S_{qd} = \sum_i \max_j (E_{q_i}^\top E_{d_j})
          \]
        </p>
        <p>
          Documents are sorted by these scores, and the top-k are returned as final results.
        </p>

        <h3>ColBERTv2</h3>
        <p>
          ColBERTv2 improves ColBERT with new training strategies and compressed indexing.
          It uses knowledge distillation and efficient candidate generation with reduced inverted lists.
        </p>
      </section>

      <section id="llm-rerank">
        <h2>LLM-Based Re-ranking</h2>
        <h3>RankGPT</h3>
        <p>
          Instead of traditional supervised fine-tuning, RankGPT uses <strong>permutation knowledge distillation</strong>
          with two innovations: sliding window data generation and permutation distillation with RankLoss.
        </p>

        <h4>Sliding Window</h4>
        <p>
          To overcome token limits of LLMs, passages are re-ranked in batches using a sliding window.
          The window moves backwards through the list, locking in the relative order of top documents.
        </p>

        <h4>Permutation Distillation</h4>
        <p>
          RankNet loss focuses on relative ordering of passages.
          For a query \(q\), passages \(p_1,\dots,p_M\), and ground-truth ranking \(R\), the loss is:
        </p>
        <p>
          \[
          L_{\text{RankNet}} = \sum_{i=1}^M \sum_{j=1}^M \mathbf{1}_{r_i < r_j} \log(1 + \exp(s_j - s_i))
          \]
        </p>
        <p>
          Here, \(s_i = f_\theta(q, p_i)\) is the predicted score.  
          The loss penalizes incorrect orderings (inversions), ensuring the student model matches the teacher’s ranking.
        </p>
      </section>

      <section id="api-rerank">
        <h2>API-Based Models</h2>
        <p>
          API services such as <strong>Cohere Rerank</strong> provide easy-to-integrate, production-ready solutions.
          They are optimized for speed and reliability but add third-party dependency and recurring cost.
        </p>
      </section>
</main>


    
<footer class="page__meta">
  <!-- Any additional metadata or footer information goes here -->
</footer>

<div class="page__footer">
  <footer>
    <!-- Your footer content goes here -->
    <p>@2025 Margi Pandya</p>
  </footer>
</div>

</body>
</html>
